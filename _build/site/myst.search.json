{"version":"1","records":[{"hierarchy":{"lvl1":"Molecular descriptors and similarity"},"type":"lvl1","url":"/descriptors-similarity","position":0},{"hierarchy":{"lvl1":"Molecular descriptors and similarity"},"content":"","type":"content","url":"/descriptors-similarity","position":1},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl2":"Molecular descriptors"},"type":"lvl2","url":"/descriptors-similarity#molecular-descriptors","position":2},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl2":"Molecular descriptors"},"content":"Molecular descriptors provide a way of encoding information about the structure or properties of a molecule in a primarily numerical form.\n\nRoberto Todeschini and Viviana Consonni who literally wrote several books on molecular descriptors (including \n\nthis one [1] which you can access through UoL library) defined molecular descriptors as follows:\n\n“The molecular descriptor is the final result of a logic and mathematical procedure which transforms chemical information\nencoded within a symbolic representation of a molecule into a useful number, or the result of some standardized experiment.”[2]","type":"content","url":"/descriptors-similarity#molecular-descriptors","position":3},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Calculating descriptors","lvl2":"Molecular descriptors"},"type":"lvl3","url":"/descriptors-similarity#calculating-descriptors","position":4},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Calculating descriptors","lvl2":"Molecular descriptors"},"content":"Molecular descriptors are usually calculated based on a molecular representation [3], like a SMILES string or a molecular graph.\n\nYou have worked with some notation systems to represent molecules, such as SMILES and InChI. The distinction between a molecular representation and some types of molecular descriptors that encode structural information like molecular fingerprints can seem unclear. The table below summarises the information and how it is expressed and used for different types of molecular representations and descriptors.\n\nFeature\n\nString-Based Representation\n\nMolecular Graph (Representation)\n\nMolecular Fingerprint (Descriptor)\n\nProperty Descriptor\n\nDefinition\n\nLinear or hierarchical text notation of molecular structure\n\nGraph-based connectivity model\n\nEncoded numerical feature set\n\nNumerical value representing physical, chemical or biological property\n\nFormat\n\nText string (ASCII)\n\nNodes (atoms) and edges (bonds)\n\nBit vector, integer array\n\nScalar, vector, or matrix\n\nExamples\n\nSMILES (CCO), InChI (InChI=1S/C2H6O/...)\n\nAdjacency matrix, edge list\n\nMACCS keys, ECFP, Morgan FP\n\nLogP, dipole moment, polar surface area (PSA)\n\nTypical use\n\nDatabase searching, input for descriptor calculations or other structure-based methods\n\nInput for descriptor calculations\n\nSimilarity searching, QSAR, clustering\n\nQSAR/QSPR, toxicity prediction, virtual screening\n\nInformation Captured\n\nAtomic connectivity in text format\n\nTopological information, connectivity\n\nSubstructural patterns, fragments\n\nUsually computed, but also experimental molecular properties","type":"content","url":"/descriptors-similarity#calculating-descriptors","position":5},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Molecular descriptors classed by dimension","lvl2":"Molecular descriptors"},"type":"lvl3","url":"/descriptors-similarity#molecular-descriptors-classed-by-dimension","position":6},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Molecular descriptors classed by dimension","lvl2":"Molecular descriptors"},"content":"Descriptors are often described in terms of their dimensionality, which depends on the level of information about the molecule required to generate the descriptor. For example, a 0D descriptor like molecular weight or the number of atoms present can be calculated just from the molecular formula; whereas a 1D property like the number of hydrogen bond donors required some information about the molecule’s connectivity.\n\nDimensionality\n\nWhat It Captures\n\nExamples\n\n0D (Constitutional Descriptors)\n\nAtomic composition only, no bonding info\n\nMolecular weight, atom counts (C, H, O, etc.), molecular formula\n\n1D (Global Properties)\n\nMolecular properties\n\nNumber of rings, LogP, number of H-bond donors\n\n2D (Topological Descriptors)\n\nConnectivity, graph-based structure\n\nWiener Index, Morgan fingerprint, number of rotatable bonds\n\n3D (Geometric & Electronic Descriptors)\n\n3D shape, spatial arrangement\n\nTPSA, dipole moment, molecular volume\n\n4D (Conformational Ensembles)\n\nMultiple conformations, flexible structures\n\nMolecular interaction fields, ensemble pharmacophores\n\nThere are many many different descriptors available. The Todeschini and Consonni book \n\nMolecular Descriptors for Cheminformatics [1] is an encyclopedic reference on descriptors, primarily those relevant in medicinal chemistry or drug discovery-related areas.\n\nOne type of descriptor that finds broad use and has become extremely prevalent due to the rising application of AI and machine learning are molecular fingerprint. This is a very brief introduction to what they are and how they can be used for measuring the similarity of chemical compounds.","type":"content","url":"/descriptors-similarity#molecular-descriptors-classed-by-dimension","position":7},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Molecular fingerprints","lvl2":"Molecular descriptors"},"type":"lvl3","url":"/descriptors-similarity#molecular-fingerprints","position":8},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Molecular fingerprints","lvl2":"Molecular descriptors"},"content":"A molecular fingerprint is a type of descriptor (rather than a representation).\n\nIt is a computed numerical encoding of a molecule’s structure, typically represented as a bit vector or hashed feature vector.\n\nUnlike a molecular graph, which represent connectivity, fingerprints transform structural information into a numerical format for use in QSAR modeling, similarity searching, and machine learning.\n\nFingerprints are derived from molecular structure, making them descriptors rather than raw representations.","type":"content","url":"/descriptors-similarity#molecular-fingerprints","position":9},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl4":"Types of Molecular Fingerprints (a non-exhaustive list)","lvl3":"Molecular fingerprints","lvl2":"Molecular descriptors"},"type":"lvl4","url":"/descriptors-similarity#types-of-molecular-fingerprints-a-non-exhaustive-list","position":10},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl4":"Types of Molecular Fingerprints (a non-exhaustive list)","lvl3":"Molecular fingerprints","lvl2":"Molecular descriptors"},"content":"Structural (key-based) fingerprints\n\nEncode presence or absence of substructures specified in a pre-defined library.\n\nExample: MACCS keys.\n\n\n\nPath-based fingerprints\n\nCompress all (usually linear) paths in a molecule up to a specified length into a fixed-length bit vector using hashing.\n\nExample: Daylight Fingerprints.\n\nCircular (radial) fingerprints\n\nCapture atomic environments iteratively (e.g., \n\nExtended Connectivity Fingerprint ECFP [4] - nice primer \n\nhere).\n\nExample: Morgan fingerprints - probably the most prevalent molecular fingerprint currently.\n\nPharmacophore-based fingerprints\n\nEncode molecular features relevant to binding (e.g., hydrogen bond donors/acceptors, hydrophobicity) [5].\n\nExample: PH4 fingerprints.\n\nSome other resources to learn about fingerprints\n\nThere is an intro to some of the molecular fingerprints available in the RDKit in the \n\nRDKit book and docs.\n\nThis \n\nRDKit blog post details how to generate various types of fingerprint in the RDKit.\nThe RDKit \n\nGetting Started guide also has info on fingerprints and similarity (see later section), and \n\nunderstanding what bits in the fingerprints mean with some visualisations.","type":"content","url":"/descriptors-similarity#types-of-molecular-fingerprints-a-non-exhaustive-list","position":11},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl2":"Molecular Similarity"},"type":"lvl2","url":"/descriptors-similarity#molecular-similarity","position":12},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl2":"Molecular Similarity"},"content":"One of the major uses of molecular fingerprints is to assess molecular similarity by comparing the bit vectors (or count vectors) of different molecules.","type":"content","url":"/descriptors-similarity#molecular-similarity","position":13},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"How does molecular similarity work with fingerprints?","lvl2":"Molecular Similarity"},"type":"lvl3","url":"/descriptors-similarity#how-does-molecular-similarity-work-with-fingerprints","position":14},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"How does molecular similarity work with fingerprints?","lvl2":"Molecular Similarity"},"content":"Generate fingerprints\n\nConvert molecules into fingerprints (e.g., Morgan fingerprints (ECFP), MACCS keys).\n\nCompute a similarity metric to compare fingerprints\n\nThe Tanimoto coefficient (also called Jaccard similarity) is the most common measure used in combination with fingerprints\n\nOther similarity measure include Dice similarity, Cosine similarity, and Euclidean distance (see below)\n\nInterpret similarity scores\n\nDecide what values of score indicates if molecule is similar or different enough.\n\nHow similar is similar? [6] \n\nA Med Chem perspective [7]\n\nHow similar two compounds (or other chemical entities) can be a rather contentious question, but it is at the heart of many areas of discovery and design in chemical and molecular sciences, even in processes as seemingly simple as database searching. Quantitative measures based on encoded representations or fingerprints are therefore essential, but it can be challenging to distil “similarity” down to a single measure.","type":"content","url":"/descriptors-similarity#how-does-molecular-similarity-work-with-fingerprints","position":15},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Similarity measures","lvl2":"Molecular Similarity"},"type":"lvl3","url":"/descriptors-similarity#similarity-measures","position":16},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Similarity measures","lvl2":"Molecular Similarity"},"content":"A variety of similarity measures are used in different applications and fields [8], and the choice of a particular measure might be dependent various factors, such as easy they are to compute, the information being compared and the type of similarity - or difference - they are assessing [7].\n\nMany studies show that similarity coefficient can also show different sensitivities depending on the representation or fingerprint on which they were calculated and the combination of representation/fingerprint and similarity measure might be more or less appropriate in different contexts [9-13].\n\na = number of bits on in fingerprint A (i.e., |A|)\n\nb = number of bits on in fingerprint B (i.e., |B|)\n\nc = number of shared “on” bits between A and B (i.e., |A \\cap B|)\n\nSimilarity Measure\n\nFormula\n\nTanimoto-Jaccard coefficent\n\nS_{AB} = \\frac{c}{a + b - c}\n\nDice coefficent\n\nS_{AB} = \\frac{2c}{a + b}\n\nCosine coefficent\n\nS_{AB} = \\frac{c}{\\sqrt{a b}}\n\nEuclidean distance\n\nD_{AB} = \\sqrt{a + b - 2c}\n\nHamming (Manhattan) distance\n\nD_{AB} = a + b - 2c\n\nTversky coefficent\n\nS_{AB} = \\frac{c}{\\alpha (a - c) + \\beta (b - c) + c}\n\nS_{AB} are similarity metrics; \n\nD_{AB} are distance or dissimilarity measures.","type":"content","url":"/descriptors-similarity#similarity-measures","position":17},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl4":"Example applications","lvl3":"Similarity measures","lvl2":"Molecular Similarity"},"type":"lvl4","url":"/descriptors-similarity#example-applications","position":18},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl4":"Example applications","lvl3":"Similarity measures","lvl2":"Molecular Similarity"},"content":"Virtual Screening: Find drug-like molecules similar to known active compounds.\n\nClustering: Group similar molecules in chemical databases.\n\nQSAR Modeling: Identify structurally similar molecules with similar biological activity.\n\nDiversity Analysis: Assess chemical diversity in compound libraries.","type":"content","url":"/descriptors-similarity#example-applications","position":19},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Summary","lvl2":"Molecular Similarity"},"type":"lvl3","url":"/descriptors-similarity#summary","position":20},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"Summary","lvl2":"Molecular Similarity"},"content":"Molecular descriptors and similarity measures are central to data-driven chemistry, enabling drug design, materials discovery, and predictive modelling.\n\nNo single fingerprint or similarity measure works universally across all fields, necessitating custom approaches for complex systems like crystals [14-16], polymers and soft matter [17, 18]. Graph-based methods and machine learning are key frontiers in tackling these challenges, offering new ways to encode and compare complex chemical structures.","type":"content","url":"/descriptors-similarity#summary","position":21},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"For you to consider","lvl2":"Molecular Similarity"},"type":"lvl3","url":"/descriptors-similarity#for-you-to-consider","position":22},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl3":"For you to consider","lvl2":"Molecular Similarity"},"content":"How applicable are the kind of molecular descriptors discussed here to inorganic compounds?\n\nWhat kinds of issues might need to be factored into representations or descriptors for extended materials?\n\nAre there particular considerations for polymers vs. crystalline solids?\n\nAre structure and property/activity related in the same way for materials as they are for (for example) bioactive molecules?","type":"content","url":"/descriptors-similarity#for-you-to-consider","position":23},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl4":"References","lvl3":"For you to consider","lvl2":"Molecular Similarity"},"type":"lvl4","url":"/descriptors-similarity#references","position":24},{"hierarchy":{"lvl1":"Molecular descriptors and similarity","lvl4":"References","lvl3":"For you to consider","lvl2":"Molecular Similarity"},"content":"R. Todeschini and V. Consonni, Molecular Descriptors for Cheminformatics, Wiley-VCH, Weinheim, 2009. \n\nTodeschini & Consonni (2009)\n\nR. Todeschini and V. Consonni, Handbook of Molecular Descriptors, Wiley-VCH, Weinheim, 2000. \n\nTodeschini & Consonni (2000)\n\n1 D. S. Wigh, J. M. Goodman and A. A. Lapkin, A review of molecular representation in the age of machine learning, WIREs Comput Mol Sci, 2022, 12, e1603. \n\nWigh et al. (2022)\n\nD. Rogers and M. Hahn, Extended-Connectivity Fingerprints, J. Chem. Inf. Model., 2010, 50, 742–754. \n\nRogers & Hahn (2010)\n\nM. J. McGregor and S. M. Muskal, Pharmacophore Fingerprinting. 1. Application to QSAR and Focused Library Design, J. Chem. Inf. Comput. Sci., 1999, 39, 569–574. \n\nMcGregor & Muskal (1999)\n\nD. E. Patterson, R. D. Cramer, A. M. Ferguson, R. D. Clark and L. E. Weinberger, Neighborhood Behavior:  A Useful Concept for Validation of “Molecular Diversity” Descriptors, J. Med. Chem., 1996, 39, 3049–3059. \n\nPatterson et al. (1996)\n\nG. Maggiora, M. Vogt, D. Stumpfe and J. Bajorath, Molecular Similarity in Medicinal Chemistry: Miniperspective, J. Med. Chem., 2014, 57, 3186–3204. \n\nMaggiora et al. (2014)\n\n1 P. Willett, J. M. Barnard and G. M. Downs, Chemical Similarity Searching, J. Chem. Inf. Comput. Sci., 1998, 38, 983–996. \n\nWillett et al. (1998)\n\n1 D. Bajusz, A. Rácz and K. Héberger, Why is Tanimoto index an appropriate choice for fingerprint-based similarity calculations?, J. Cheminform., 2015, 7, 20. \n\nBajusz et al. (2015)\n\n1 D. Stumpfe and J. Bajorath, Similarity searching, Wiley Interdiscip. Rev. Comput. Mol. Sci., 2011, 1, 260–282. \n\nStumpfe & Bajorath (2011)\n\n1 J. Bajorath, Molecular crime scene investigation – dusting for fingerprints, Drug Discovery Today: Technologies, 2013, 10, e491–e498. \n\nBajusz et al. (2015)\n\n1 R. Duke, C.-H. Yang, B. Ganapathysubramanian and C. Risko, Evaluating molecular similarity measures: Do similarity measures reflect electronic structure properties?, ChemRxiv, 2025, preprint. \n\nDuke et al. (2025).\n\n1 C. L. Mellor, R. L. Marchese Robinson, R. Benigni, D. Ebbrell, S. J. Enoch, J. W. Firman, J. C. Madden, G. Pawar, C. Yang and M. T. D. Cronin, Molecular fingerprint-derived similarity measures for toxicological read-across: Recommendations for optimal use, Regul. Toxic. Pharmacol., 2019, 101, 121–134. \n\nMellor et al. (2019)\n\n1 A. P. Bartók, R. Kondor and G. Csányi, On representing chemical environments, Phys. Rev. B, 2013, 87, 184115. \n\nBartók et al. (2013).\n\nK. T. Schütt, How to represent crystal structures for machine learning: Towards fast prediction of electronic properties, Phys. Rev. B, 2014, 89 \n\nSchütt (2014).\n\nL. Himanen, M. O. J. Jäger, E. V. Morooka, F. Federici Canova, Y. S. Ranawat, D. Z. Gao, P. Rinke and A. S. Foster, DScribe: Library of descriptors for machine learning in materials science, Computer Physics Communications, 2020, 247, 106949. \n\nHimanen et al. (2020)\n\nS. Stuart, J. Watchorn and F. X. Gu, Sizing up feature descriptors for macromolecular machine learning with polymeric biomaterials, npj Comput Mater, 2023, 9, 1–10. \n\nStuart et al. (2023)\n\nY. Zhao, R. J. Mulder, S. Houshyar and T. C. Le, A review on the application of molecular descriptors and machine learning in polymer design, Polym. Chem., 2023, 14, 3325–3346. \n\nZhao et al. (2023)","type":"content","url":"/descriptors-similarity#references","position":25},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints"},"type":"lvl1","url":"/fingerprints","position":0},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints"},"content":"Molecular descriptors provide a way of encoding information about the structure or properties of a molecule in a primarily numerical form.\n\nRoberto Todeschini and Viviana Consonni who literally wrote several books on molecular descriptors (including \n\nthis one which you can access through UoL library) defined molecular descriptors as follows:\n\n“The molecular descriptor is the final result of a logic and mathematical procedure which transforms chemical information\nencoded within a symbolic representation of a molecule into a useful number, or the result of some standardized experiment.”[1]\n\nSee Also\n\nSome background on molecular descriptors, fingerprints and similarity","type":"content","url":"/fingerprints","position":1},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Aim of this notebook exercise"},"type":"lvl3","url":"/fingerprints#aim-of-this-notebook-exercise","position":2},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Aim of this notebook exercise"},"content":"This notebook will look at an example of generating and visualising Morgan fingerprints - a 2D descriptor that is type of circular topological fingerprint - for some relatively simple molecules using RDKit. We can then use the fingerprint to assess their similarity using the Tanimoto-Jaccard coefficient.\n\nYou saw how to calculate a Morgan fingerprint when you covered FAIR data in CHEM502, so some of this will be a recap and give you some practice wth RDKit.\n\nNote\n\nTo see the complete notebook, click \n\nhere.\n\n# import statements - make sure to run this cell\nimport requests\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nfrom IPython.display import SVG\n\nfrom rdkit import Chem\nfrom rdkit.Chem import (\n                        AllChem,\n                        rdCoordGen,\n                        Draw,\n                        rdFingerprintGenerator,\n                        PandasTools,\n                        Descriptors\n                        )\n# from rdkit.Chem.Draw import rdMolDraw2D, \n# from rdkit.Chem.Draw import MolsToGridImage\nfrom rdkit.Chem.Draw import IPythonConsole\nfrom rdkit import DataStructs\n\nfrom IPython.display import SVG\nfrom ipywidgets import interact,fixed,IntSlider\n\n\n\n\nIPythonConsole.ipython_useSVG=True\n\n\n\n\n","type":"content","url":"/fingerprints#aim-of-this-notebook-exercise","position":3},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"RDKit"},"type":"lvl3","url":"/fingerprints#rdkit","position":4},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"RDKit"},"content":"RDKit docs\n\nRDKit [2] is an open-source cheminformatics toolkit used for handling chemical structures, molecular representations and computational chemistry tasks. It includes tools for descriptor calculation, generating molecular fingerprints, similarity searching, virtual screening and QSAR modelling.\n\nOne of RDKit’s key capabilities is computing a wide variety of molecular descriptors. You covered the basics of using RDKit in the FAIR data workshop in CHEM501. There are also some resources below that may be useful, particularly for calculating descriptors in RDKit.\n\nThe RDKit Cookbook\n\nList of available descriptors in the RDKit\n\nCalculating descriptors using RDKit\n\nFingerprints in the RDKit\n\n","type":"content","url":"/fingerprints#rdkit","position":5},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Morgan (circular) fingerprints for some simple-ish example molecules"},"type":"lvl3","url":"/fingerprints#morgan-circular-fingerprints-for-some-simple-ish-example-molecules","position":6},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Morgan (circular) fingerprints for some simple-ish example molecules"},"content":"Molecular fingerprints are an abstract representation of features in a molecule, and can seem very abstract indeed. RDKit provides some methods to visualise how the bits in a fingerprint relate to fragments in the molecule, which can help to understand what the fingerprint actually represents.\n\nThere is a brief intro to Morgan fingerprints in RDKit in the \n\nRDKit Getting Started guide.\n\nIn the cell below, there are some helper functions for generating and visualising the RDKit molecules and fingerprints.\n\n# Helper functions for molecule and fingerprint generation and visualisation.\ndef get_2D_molecule(mol: Chem.Mol, addHs: bool=False) -> Chem.Mol:\n    \"\"\"Get rdkit.Mol with 2D coords\"\"\"\n\n    if addHs:\n        mol = Chem.AddHs(mol) if addHs else mol     \n    rdCoordGen.AddCoords(mol)\n    return mol\n\ndef get_3D_molecule(mol: Chem.Mol) -> Chem.Mol:\n    \"\"\"Get rdkit.Mol with 3D coords\"\"\"\n\n    mol = Chem.AddHs(mol)\n    AllChem.EmbedMolecule(mol)\n    return mol\n\ndef get_molecule_from_smiles(smiles: str, addHs: bool=False, make3D: bool=False) -> Chem.Mol:\n    \"\"\"Get rdkit.Mol from SMILES\"\"\"\n    \n    mol =  Chem.MolFromSmiles(smiles)\n    if make3D:\n        mol = get_3D_molecule(mol)\n    else:\n        mol = get_2D_molecule(mol, addHs=addHs)\n    return mol\n\ndef draw_2D_molecule(mol: Chem.Mol, addLabels: bool=False, forFP: bool=False) -> Draw.MolDraw2DSVG:\n    \"\"\"Draw 2D molecule\"\"\"\n    \n    d2d = Draw.MolToImage(mol)\n    if addLabels:\n        for atom in mol.GetAtoms():\n            # For each atom, set the property \"atomNote\" to a index+1 of the atom\n            if forFP:\n                atom.SetProp(\"atomNote\", str(atom.GetIdx()))\n            else:\n                atom.SetProp(\"atomNote\", str(atom.GetIdx()+1))    \n    return d2d\n\n\ndef get_Morgan_fingerprint_with_bits(mol: Chem.Mol, radius: int=2, fpSize: int=2048) -> tuple:\n    \"\"\"Get Morgan fingerprint and its bit information map\"\"\"\n\n    mfp_gen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=fpSize)\n    ao = rdFingerprintGenerator.AdditionalOutput()\n    ao.CollectBitInfoMap()\n    mfp = mfp_gen.GetFingerprint(mol, additionalOutput=ao)\n    bit_info = ao.GetBitInfoMap()\n    return mfp, bit_info \n\ndef get_Morgan_fingerprint_with_ao(mol, radius=2, fpSize=2048):\n    \"\"\"Get Morgan fingerprint and its additional output\"\"\"\n    mfp_gen = rdFingerprintGenerator.GetMorganGenerator(radius=radius,fpSize=fpSize)\n    \n    ao = rdFingerprintGenerator.AdditionalOutput()\n    ao.AllocateAtomCounts()\n    ao.AllocateAtomToBits()\n    ao.AllocateBitInfoMap()\n    \n    mfp = mfp_gen.GetFingerprint(mol, additionalOutput=ao)\n    return mfp, ao\n\ndef draw_Morgan_fps_with_bits(mol: Chem.Mol, mfp, bit_info: dict) -> Draw.MolDraw2DSVG:\n    \"\"\"Get SVG drawer for grid of Morgan fingerprints with bits highlighted\"\"\"\n\n    on_bits_list = [(mol, bit_idx, bit_info, atom_info) for bit_idx in mfp.GetOnBits() for atom_info in range(len(bit_info[bit_idx]))]\n    labels = [f\"Bit {str(i[1])}\" for i in on_bits_list]\n    d = Draw.DrawMorganBits(on_bits_list, molsPerRow=5, legends=labels)  # Draw the on bits\n    return d\n\ndef save_Morgan_fps_with_bits(mol, mfp, bit_info) -> Draw.MolDraw2DSVG:\n    \"\"\"Get drawer of grid of Morgan fingerprints with bits highlighted to save as png\"\"\"\n\n    on_bits_list = [(mol, bit_idx, bit_info, atom_info) for bit_idx in mfp.GetOnBits() for atom_info in range(len(bit_info[bit_idx]))]\n    labels = [f\"Bit {str(i[1])}\" for i in on_bits_list]\n    drawOptions = Draw.rdMolDraw2D.MolDrawOptions()\n    drawOptions.prepareMolsBeforeDrawing = False\n    drawOptions.legendFontSize = 60\n    d = Draw.DrawMorganBits(on_bits_list, molsPerRow=5, legends=labels, subImgSize=(300, 300), useSVG=False, drawOptions=drawOptions)  # Draw the on bits\n    return d\n\n\ndef renderFpBit(mol, bit_idx, bit_info,fn):\n    return (display(fn(mol, bit_idx, bit_info)))\n\ndef draw_interactive_Morgan_fps_with_bits(mol, bit_info):\n    \"\"\"Draw Morgan fingerprints with bits highlighted with interactive bit selection\"\"\"\n\n    interact(renderFpBit, bit_idx=list(bit_info.keys()),mol=fixed(mol),\n            bit_info=fixed(bit_info),fn=fixed(Draw.DrawMorganBit));\n\n\n\n","type":"content","url":"/fingerprints#morgan-circular-fingerprints-for-some-simple-ish-example-molecules","position":7},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"Find SMILES for some common compounds","lvl3":"Morgan (circular) fingerprints for some simple-ish example molecules"},"type":"lvl4","url":"/fingerprints#find-smiles-for-some-common-compounds","position":8},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"Find SMILES for some common compounds","lvl3":"Morgan (circular) fingerprints for some simple-ish example molecules"},"content":"To start with, we will acquire the SMILES strings of some well known compounds. In case you need a SMILES recap: \n\nDaylight theory, \n\nquick tutorial.\n\nThe \n\nChemical Identifier Resolver (CIR) service is run by the CADD Group at the NCI/NIH as part of their \n\nCactus server.\n\nIn addition to its \n\nweb interface, it has an easy-to-use URL API: You can supply a chemical identifier and requests that it returns a representation of a specified type as a string. As well as allowing searches by various molecular representations, you can also search by and for IUPAC Names and even non-standard names. WYou can see the in/output formats it can handle on the web interface.\n\nCactus API urls have the form: https://cactus.nci.nih.gov/chemical/structure/<structure identifier>/<representation> so we can use the requests library to programmatically access representations for compounds.\n\nYou used requests for API access in CHEM502. Check the \n\nquickstart guide for a reminder.\n\nROOT_URL = \"https://cactus.nci.nih.gov/chemical/structure/\"\n\n# For example, to retrieve the InChIKey for the structure with the SMILES \"c1ccccc1\", we would use the following URL:\nidentifier = \"c1ccccc1\"\nrepresentation = \"stdinchikey\"\n\nquery_url = f\"{ROOT_URL}{identifier}/{representation}\"\n\nresponse = requests.get(query_url)\n\nif response:\n    print(response.text)\nelse:\n    raise Exception(f\"Cactus request failed: {response.status_code}\")\n\n\n\n\n\n# Let's try getting a SMILES representation based on a common, non-systematic name for a compound.\n\nidentifier = \"aspirin\"\nrepresentation = \"smiles\"\n\nquery_url = f\"{ROOT_URL}{identifier}/{representation}\"\n\nresponse = requests.get(query_url)\nif response:\n    print(response.text)\nelse:\n    raise Exception(f\"Cactus request failed: {response.status_code}\")\n\n\n\n\n\n# We can create an RDKit molecule from the SMILES string and visualise to check it is actually aspirin.\n# In 2D, ChemDraw graph style:\n\naspirin = Chem.MolFromSmiles(response.text)\naspirin\n\n\n\n# And a 3D view:\nIPythonConsole.drawMol3D(aspirin)\n\n\n\nDoes it agree with what Wikipedia thinks aspirin looks like?\n\nAspirin on wiki\n\nIt looks like that is the correct SMILES for aspirin.\n\n# Here is a function so the process of getting the SMILES can be repeated for multiple compounds.\n# It includes a sleep time (`time.sleep`) to avoid overloading the server.\n\ndef get_smiles_from_name(name):\n\n    # TODO: Write a function that retrieves the SMILES string using the Cactus API\n    # to the CIR service for a compound based on its common name and returns the SMILES\n    # string as text.\n\n    pass\n\n\n\n\n\ncompounds = [\"epinephrine\", \"ibuprofen\", \"dopamine\", \"caffeine\", \"naproxen\", \"paracetamol\", \"paraxanthine\", \"vanillin\",\n             \"adenosine\", \"aspirin\", \"niacinamide\", \"theobromine\", \"diclofenac\", \"theophylline\", \"amphetamine\"]\n\ncompounds_smiles = {compound: get_smiles_from_name(compound) for compound in compounds}\ncompounds_smiles\n\n\n\n# Visualise the molecules\n\nmols = [Chem.MolFromSmiles(smiles) for smiles in compounds_smiles.values()]\nDraw.MolsToGridImage(mols, molsPerRow=5, legends=compounds_smiles.keys())\n\n\n\n","type":"content","url":"/fingerprints#find-smiles-for-some-common-compounds","position":9},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Generating a Morgan fingerprint"},"type":"lvl3","url":"/fingerprints#generating-a-morgan-fingerprint","position":10},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Generating a Morgan fingerprint"},"content":"We will use niacinamide to see an example of generating its Morgan fingerprint and looking at the information encoding in its bit vector.\n\nniacinamide = get_molecule_from_smiles(compounds_smiles[\"niacinamide\"])\nniacinamide\n\n\n\nThe Morgan fingerprint (MFP) is a type of circular fingerprint that encodes the presence of substructures in the molecule.\n\nThe fingerprint can be obtained as a bit vector (a binary array of 0s and 1s) or as a set of counts.\n\nIn the bit vector, an ‘on’ bit, i.e. a “1”, indicates the presence of a particular substructure in the molecule.\n\nThe count-based fingerprint also stores the frequency that the substructure occurs.\n\nHere we will see the fingerprint as a bit vector. You can see the function used to generate the MFP further up in the notebook. It uses an RDKit \n\nMorgan fingerprint generator to create a fingerprint of a molecule.\n\nThe function then returns the fingerprint and a bit information map that can be used to highlight the substructures in the molecule that correspond to the bits in the fingerprint.\n\nniacinamide_mfp, niacinamide_bi = get_Morgan_fingerprint_with_bits(niacinamide)\ndisplay(type(niacinamide_mfp), type(niacinamide_bi))\n\n\n\n\n\n# Display the fingerprint in the form of a bit matrix (the array of 2048 bits is split into 16 rows of 128 bits just to dispaly more easily)\n\nbits = np.array(niacinamide_mfp).reshape(16,128)\nfor row in bits:\n    print(*row)\n\n# show the number of bits that are on\nprint(f\"Number of on bits (value is 1): {niacinamide_mfp.GetNumOnBits()}\")\n\n\n\n","type":"content","url":"/fingerprints#generating-a-morgan-fingerprint","position":11},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Interpreting bit information"},"type":"lvl3","url":"/fingerprints#interpreting-bit-information","position":12},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl3":"Interpreting bit information"},"content":"The bit information map is a dictionary where the keys are the bit indices and the values are sets of pair tuples describing the fragment that contribute to that bit.\n\nThe pair tuples are of the form (atomId, radius) where atomId is the index of central atom and the radius is the extent of the circle (number of bonds from the central atom) covering the fragment.\n\ndisplay(draw_2D_molecule(niacinamide, forFP=True, addLabels=True))\n\nniacinamide_bi\n\n\n\nFrom the niacinamide MFP, the first entry in bit info dictionary is for bit 140:140: ((0, 1),)\n\nStarting at the atom at index 0 - the diagram shows this is the primary amine nitrogen - and drawing a circle that encloses atoms one bond away locates a pattern in the molecule that matches whatever substructure is represented by bit 140.\n\nYou can see there can be some bits where more than atom’s environment also contributes to the same bit (bit 1873 for niacinamide). This is a \n\nbit collision and can cause some information loss which could affect further computations based on the fingerprint - particularly relevant for machine learning performance - if the collisions are extensive.\n\nMaking the fingerprint larger can help to reduce bit collisions, but comes with computational expense. Using a count-based MFP rather than the bit vector means that information about the number of number of atom environments for a bit are retained, so count-based MFPs may be preferred if the fingerprint will be used in ML.\n\n","type":"content","url":"/fingerprints#interpreting-bit-information","position":13},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"Visualising bits in the Morgan fingerprint","lvl3":"Interpreting bit information"},"type":"lvl4","url":"/fingerprints#visualising-bits-in-the-morgan-fingerprint","position":14},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"Visualising bits in the Morgan fingerprint","lvl3":"Interpreting bit information"},"content":"The parts of the molecule contributing to the ‘on’ bits (bits with a value of 1) can also be visualised.\n\nThe colors of the Morgan bits’ highlights indicate the nature of the atoms in the neighbouring environment of the central atom. The radius for this fingerprint was set to 2, so it considers the local environment of the central atom up to atoms two bonds away.\n\nBlue - central atom in the environment\n\nYellow - aromatic atoms\n\nGrey - aliphatic ring atoms\n\n* + faint grey bonds - an atom not part of the bit, but shows the connectivity of atoms that are.\n\n# Use rdkit to draw the fragments corresponding to the on bits in the bit information map\ni = draw_Morgan_fps_with_bits(niacinamide, niacinamide_mfp, niacinamide_bi)\ni\n\n\n\nThis makes it striaghtforward to infer that bit 1873 encodes the presence of unsubtituted aromatic carbon. There may be additional criteria, but carbons 4, 5, 6, 8 all match the substructure.\n\n# img = save_Morgan_fps_with_bits(niacinamide, niacinamide_mfp, niacinamide_bi)\n# img.save(\"niacinamide_mfp.png\")\n\n\n\nShort notebook fanciness diversion\n\nYou can use ipywidgets to create a visualisation of the bits in a notebook that lets you select which bit you want to view:\n\ndraw_interactive_Morgan_fps_with_bits(niacinamide, niacinamide_bi)\n\n\n\n","type":"content","url":"/fingerprints#visualising-bits-in-the-morgan-fingerprint","position":15},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl2":"Molecular Similarity"},"type":"lvl2","url":"/fingerprints#molecular-similarity","position":16},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl2":"Molecular Similarity"},"content":"One of the major uses of molecular fingerprints is to assess molecular similarity by comparing the bit vectors (or count vectors) of different molecules.\n\nWe can generate fingerprints for our small collection of molecules, then using one as a reference, calculate a measure of how similar the other molecules are to that molecule.\n\nTo start with, we will use caffeine as the reference and locate which of the other compounds are most similar.\n\n# A quick reminder of the compounds we are working with:\n\nDraw.MolsToGridImage(mols, molsPerRow=5, legends=compounds_smiles.keys())\n\n\n\n\nThe RDKit has a module for working with molecules in Pandas, which can be particularly useful if you are dealing with a significant amount of molecules or data.\n\nPandasTools module\n\n# create a pandas DataFrame with the name and SMILES for each compound\n\nmols_df = pd.DataFrame.from_dict(compounds_smiles, orient=\"index\", columns=[\"SMILES\"]).reset_index().rename(columns={\"index\": \"name\"})\nmols_df\n\n\n\n# We could use the existing list of RDKit molecules to add a column to the DataFrame, but we will use the SMILES strings \n# to generate the molecules again to show how to use the `PandasTools` module. The \"ROMol\" column will contain the RDKit molecules.\n\nPandasTools.AddMoleculeColumnToFrame(mols_df, smilesCol=\"SMILES\")\nmols_df\n\n\n\n# We can then add the Morgan fingerprints to the DataFrame.\n\nmfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)\n\nmols_df[\"Morgan_fingerprint\"] = mols_df[\"ROMol\"].map(lambda x: mfpgen.GetFingerprint(x))\nmols_df\n\n\n\n# Finally, we can calculate the Tanimoto similarity between the caffeine molecule and each of the other molecules in the DataFrame.\n\nmols_df[\"Tanimoto_similarity_caffeine\"] = mols_df[\"Morgan_fingerprint\"].map(lambda x: DataStructs.TanimotoSimilarity(mols_df[\"Morgan_fingerprint\"][3], x))\n\nmols_df\n\n\n\n# To see which molecule is most similar to caffeine, we can sort the DataFrame by the Tanimoto similarity.\n\nmols_df.sort_values(\"Tanimoto_similarity_caffeine\", ascending=False)\n\n\n\nThe Tanimoto similarity coefficient shows that theobromine, paraxanthine and theophylline are clearly much more similar to caffeine than the other compounds in the set.\n\nIs this what you expect just from considering the molecular structures?\n\nDo you know anything else about this set of molecules that explains why they are quite closely related?","type":"content","url":"/fingerprints#molecular-similarity","position":17},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"For you to try:","lvl2":"Molecular Similarity"},"type":"lvl4","url":"/fingerprints#for-you-to-try","position":18},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"For you to try:","lvl2":"Molecular Similarity"},"content":"Try changing the radius over which the Morgan fingerprint is calculated (the value is passed when you create the fingerprint generator). What difference does this make to the Tanimoto score calculated for the new fingerprint? Can you explain why?\n\nTry generating a different type of fingerprint, e.g. an RDKit fingerprint, and see how different the Tanimoto coefficients for that type of fingerprint are.\n\nRDKit has drawing tools to draw \n\nSimilarity Maps. See if you can generate a map to compare caffeine with one of the related compounds.\n\nChoose a different compound as the reference and to identify any other groups of related molecules in the set.\n\n","type":"content","url":"/fingerprints#for-you-to-try","position":19},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"References","lvl2":"Molecular Similarity"},"type":"lvl4","url":"/fingerprints#references","position":20},{"hierarchy":{"lvl1":"Notebook exercise - molecular fingerprints","lvl4":"References","lvl2":"Molecular Similarity"},"content":"R. Todeschini and V. Consonni, Handbook of Molecular Descriptors, Wiley-VCH, Weinheim, 2000. \n\nTodeschini & Consonni (2000)\n\nRDKit: Open-source cheminformatics. \n\nhttps://​www​.rdkit​.org\n\n","type":"content","url":"/fingerprints#references","position":21},{"hierarchy":{"lvl1":"Least squares optimisation"},"type":"lvl1","url":"/least-squares-opt","position":0},{"hierarchy":{"lvl1":"Least squares optimisation"},"content":"Least squares optimisation is used extensively in chemistry and other fields to find best-fit solutions for modelling datasets.\n\nGiven a function to model observed data, least squares methods find the best-fit solution by adjusting the model’s parameters to minimise its error. This is done by reducing the total sum of the squared differences (the residuals) between the model’s predictions and the data.\n\nTo get an idea of what is actually going on, we can look a simple case of fitting a straight line through a set of points n defined by their x and y values (x_n, y_n). These could represent set of experimental observations like the yield of a reaction as a function of reaction temperature.","type":"content","url":"/least-squares-opt","position":1},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"1. Fitting a Line Through Two Points as a System of Equations"},"type":"lvl2","url":"/least-squares-opt#id-1-fitting-a-line-through-two-points-as-a-system-of-equations","position":2},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"1. Fitting a Line Through Two Points as a System of Equations"},"content":"Let’s say we observe two data points (x_1, y_1) and (x_2, y_2).\n\nWe will assume the data follows a straight-line model:y = mx + c\n\nFor the two points, this gives two equations:m x_1 + c = y_1m x_2 + c = y_2\n\nThis is simply a system of two linear equations with two unknowns (m and c), which we can solve exactly: We use standard algebra to solve the simultaneous equations to find m and c.","type":"content","url":"/least-squares-opt#id-1-fitting-a-line-through-two-points-as-a-system-of-equations","position":3},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"2. Fitting a Line Through Two Points in Matrix Form"},"type":"lvl2","url":"/least-squares-opt#id-2-fitting-a-line-through-two-points-in-matrix-form","position":4},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"2. Fitting a Line Through Two Points in Matrix Form"},"content":"We can rewrite the same system in matrix notation:\\begin{bmatrix} x_1 & 1 \\\\ x_2 & 1 \\end{bmatrix}  \n\\begin{bmatrix} m \\\\ c \\end{bmatrix}  \n=  \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}\n\nor more generally:A x = b\n\nwhere:A = \\begin{bmatrix} x_1 & 1 \\\\ x_2 & 1 \\end{bmatrix}, \\quad\nx = \\begin{bmatrix} m \\\\ c \\end{bmatrix}, \\quad\nb = \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}\n\nSince we have exactly two equations for two unknowns, we can solve this directly using algebra (e.g. by inverting the matrix A assuming it is non-singular).","type":"content","url":"/least-squares-opt#id-2-fitting-a-line-through-two-points-in-matrix-form","position":5},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"3. What If There Are More Than Two Points?"},"type":"lvl2","url":"/least-squares-opt#id-3-what-if-there-are-more-than-two-points","position":6},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"3. What If There Are More Than Two Points?"},"content":"Now, suppose we have more than two data points (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n).\n\nWe still use the same setup for the matrices:\\begin{bmatrix} x_1 & 1 \\\\ x_2 & 1 \\\\ \\vdots & \\vdots \\\\ x_n & 1 \\end{bmatrix}  \n\\begin{bmatrix} m \\\\ c \\end{bmatrix}  \n=  \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\n\nBut now there are more equations than unknowns (this is known as an overdetermined system). It means that there is no exact solution.\n\nImagine you have done this reaction at 20 different temperatures and worked out the yield for each one. You are trying to find the line of best fit through those points. There will always be some distance between at least some of those points and whatever line you have drawn, even if the straight line model is the best representation of the relationship.\n\nInstead of finding an exact solution, we look for the best approximate solution by minimising the error between the predicted values and the observed data.\n\nSo instead solving Ax = b exactly, least squares methods do this by trying to solveA^T A x = A^T b\n\nThis equation gives the values of x (containing the parameters of our model, m and c) that minimise the error, as quantified by the sum of squared residuals (SSR) (the difference between the model and the observed data).","type":"content","url":"/least-squares-opt#id-3-what-if-there-are-more-than-two-points","position":7},{"hierarchy":{"lvl1":"Least squares optimisation","lvl4":"Direct methods","lvl2":"3. What If There Are More Than Two Points?"},"type":"lvl4","url":"/least-squares-opt#direct-methods","position":8},{"hierarchy":{"lvl1":"Least squares optimisation","lvl4":"Direct methods","lvl2":"3. What If There Are More Than Two Points?"},"content":"For some problems, it is possible to solve this equation directly.\n\nIf A^T A is invertible and it is computationally feasible, you can solve for x directly \n\nusing algebra, which is the approach used by \n\nOrdinary Least Squares (OLS). This would give us the best-fit values for m and c.\n\nOther direct methods for solving least squares problems include \n\nSingular Value Decomposition (SVD), which can improve stability when A^T A is ill conditioned. SVD also has advantages when working with noisy data, offers greater computationally efficiency and can enhance the interpretability of the results.","type":"content","url":"/least-squares-opt#direct-methods","position":9},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"4. Iterative Methods for Least Squares Optimisation"},"type":"lvl2","url":"/least-squares-opt#id-4-iterative-methods-for-least-squares-optimisation","position":10},{"hierarchy":{"lvl1":"Least squares optimisation","lvl2":"4. Iterative Methods for Least Squares Optimisation"},"content":"For some minimisation problems, computing the inverse of A^T A, i.e. (A^T A)^{-1}, can be computationally expensive (e.g. for very large datasets) or A^T A is ill-conditioned (close to singular, making it unstable to inversion). In these cases, an iterative approach can be used instead.\n\nIterative methods - as the name suggests - adjust the value of x step-by-step gradually minimising the sum of squared residuals until a minimum is located.\n\nSteepest Descent adjusts x using the gradient of the squared error. The algorithm moves in the direction largest downward - most negative - gradient to minimise the error. While effective, it can be inefficient due to slow convergence compared to other iterative methods. (“Steepest descent” and “gradient descent” are often used interchangably in some areas. Strictly, they are different as steepest descent is a special case of gradient descent. I’d suggest asking a mathematician, or ChatGPT failing that.)\n\nLevenberg-Marquardt is a more advanced technique that dynamically adjusts the step size depending on how close it is to a solution. This means it offers faster convergence than Steepest Descent. It is commonly used for nonlinear curve fitting and is the default method for least squares fitting in scipy.optimize.curve_fit due to its fast and stable performance.","type":"content","url":"/least-squares-opt#id-4-iterative-methods-for-least-squares-optimisation","position":11},{"hierarchy":{"lvl1":"Least squares optimisation","lvl3":"Key points","lvl2":"4. Iterative Methods for Least Squares Optimisation"},"type":"lvl3","url":"/least-squares-opt#key-points","position":12},{"hierarchy":{"lvl1":"Least squares optimisation","lvl3":"Key points","lvl2":"4. Iterative Methods for Least Squares Optimisation"},"content":"Fitting a line through two points is just solving two simultaneous equations.\n\nFitting a line through many points leads to an overdetermined system, where we use least squares to find the best fit.\n\nLeast squares methods find an approximate solution that minimises the error between the predictions and the observed data.\n\nDirect methods such as OLS and SVD solve a set of equations that mathematically express the least squares condition, providing the best-fit parameters in a single calculation.\n\nIterative methods offer an alternative approach by making step-by-step adjustments to the solution until the error is minimised.\n\n","type":"content","url":"/least-squares-opt#key-points","position":13},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)"},"type":"lvl1","url":"/spectrum-analysis","position":0},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)"},"content":"Experimental characterisation methods like NMR and IR commonly produce datasets in the form of 1-dimensional (or sometimes 2D) spectra. For simple 1D NMR experiments, for example, the spectrum is usually output in the form of intensity vs. chemical shift \\delta in ppm.","type":"content","url":"/spectrum-analysis","position":1},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Analysing spectra"},"type":"lvl3","url":"/spectrum-analysis#analysing-spectra","position":2},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Analysing spectra"},"content":"Determining information about the structure of molecules in the sample usually starts with extracting information about the peaks present in the NMR spectrum. The peak information can then be interpreted to provide specific information about the composition and connectivity of the molecule.\n\nIn general, fitting peaks with a function that models the shape serves several purposes including: to gain some fundamental understanding of the nature of the data (e.g. does the method and/or specific instrument produce a particular peak shape); if the peaks are modelled well, the information on their centres, amplitudes can be more precise.","type":"content","url":"/spectrum-analysis#analysing-spectra","position":3},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Aim of this notebook exercise"},"type":"lvl3","url":"/spectrum-analysis#aim-of-this-notebook-exercise","position":4},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Aim of this notebook exercise"},"content":"To get an overview how 1D spectral data can be analysed to acquire information about the peaks - positions of the centres, widths, intensities, etc. - we will look at a simple NMR spectrum and practise some pandas, matplotlib and scipy skills along the way.\n\nProcessing spectra involves modelling data. This exercise also gives us a chance to look more closely at how a model is being applied to the data and see some of the considerations and issues you should be aware of as you work with even simple data problems.\n\nNote\n\nTo see the complete notebook, click \n\nhere.\n\n# import statements\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.signal import find_peaks, peak_widths\nfrom scipy.optimize import curve_fit\n\n\n\nThe 13C NMR spectrum of ethanol is stored as a csv file (13C_EtOH.csv) in the data directory data.\n\nIt is a good idea to check the contents of a file before you try to load it using Python.\n\nThis is particularly the case with data files to get an idea of the structure of the file, whether it has any header lines that you might want to skip over or if the column names are present, for example.\n\nTip\n\nYou can run shell (terminal) command from a Jupyter notebook. More info here: \n\nhttps://​tinyurl​.com​/2yv37k2x\n\n# Use the `head` shell command to look at the first few lines of the file \n\n\n\n\n\n# Read the NMR spectrum from the csv into a pandas dataframe called nmr_spec\n\n# nmr_spec = ...\n\n\n\n\n# Uncomment line below and run cell to complete code to load the csv file\n# %load ../code_snips/load_nmr_csv.txt\n\n\n\n\n\n# Plot the spectrum\nfig, ax = plt.subplots()\nax.plot(nmr_spec[\"ppm\"], nmr_spec[\"intensity\"])\n\nplt.xlabel(\"Chemical shift $\\\\delta$ / ppm\")\nplt.ylabel(\"Intensity\")\n\nplt.xlim((70, 0))\nplt.title(r\"$\\mathregular{^{13}C}$ NMR spectrum of ethanol\")\nplt.show()\n\n\n\nThe code above uses matplotlib’s pyplot.plot to graph the spectrum. This might be how you have mostly seen plotting in matplotlib so far.\n\nPandas also provides a \n\nplot method on its DataFrame and Series objects that offers a convenient way to \n\nvisualise data using matplotlib (it uses matplotlib by default but can be changed to others, e.g. plotly).\n\nFor example, it will pick up axis labels from the DataFrame columns, but you can also modify if preferred.\n\n# Try it yourself:\n# Plot an equivalent graph of the NMR spectrum using DataFrame.plot()\n\n# Your code here...\n\n\n\n# Uncomment the line below and run the cell to see some code that works\n# %load ../code_snips/df_plot.txt\n\n\n\nDecomposing the spectrum into a set of peaks can sometimes be incorporated into the processing that is done by the experimental acquisition software. There are also various Python packages dedicated to different types of spectroscopic data which can facilitate integration into automated data processing pipelines.\n\nTo get a picture of what is going in this process, we can use some of the general methods available in scipy’s signal processing and optimisation subpackages to analyse the peaks in our simple NMR spectrum.\n\n","type":"content","url":"/spectrum-analysis#aim-of-this-notebook-exercise","position":5},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl2":"Peak processing in scipy"},"type":"lvl2","url":"/spectrum-analysis#peak-processing-in-scipy","position":6},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl2":"Peak processing in scipy"},"content":"In CHEM501, you used some of the functions available in \n\nSciPy’s \n\noptimize subpackage that can be used to fit population distributions by modelling the population distribution function (PDF) as a curve that follows a Gaussian (or Lorentzian) function.\n\nWe can use the same process to fit peaks in experimentally measured datasets like NMR spectra.\n\nThe peaks in NMR spectra are usually described as Lorentzian functions, but sometimes Gaussian or pseudo-Voigt (a mixture of Gaussian and Lorentzian) shapes are used. For our 13C NMR spectrum, we can define the Lorentzian and Gaussian functions:\n\nLorentziany = \\frac{A}{\\pi} \\frac{W/2}{(x-x_0)^2+(W/2)^2}\n\nwhere A is the amplitude of the peak, W is the full width at half maximum (FWHM) and x0 is value of x at the peak centre.\n\nGaussiany = A \\cdot \\frac{1}{\\sigma\\sqrt{2\\pi}}\\;\\exp(-\\frac{(x-x_0)^2}{2\\sigma^2})-$$y = f(x) = A \\cdot \\frac{1}{\\sigma\\sqrt{2\\pi}}\\;e^{-\\frac{(x-x_0)^2}{2\\sigma^2}}$$-\n\nHere, x0 rather than \\mu is used to represent the centre of the peak (for the normal PDF, \\mu was the mean of the  distribution) and \\sigma and the FWHM, W, of the peak are related by:W = \\sigma\\sqrt{8\\:\\ln2}\n\n# These functions will calculate a peak using a Gaussian or Lorentzian function as defined above.\n\ndef gaussian(x_array, ampl, centre, width):\n    \"\"\"Generate a signal with a Gaussian shape.\"\"\"\n    sigma = width/np.sqrt(8*np.log(2))\n    return ampl*(1/(sigma*np.sqrt(2*np.pi)))*np.exp(-(x_array-centre)**2/(2*sigma**2))\n\ndef lorentzian(x_array, ampl, centre, width):\n    \"\"\"Generate a signal with a Lorentzian shape.\"\"\"\n    h_width = width/2\n    return ampl/np.pi * h_width/((x_array-centre)**2 + h_width**2)\n\n\n\nBefore using the functions to try fitting the NMR peaks, we can look at an example of what the two peak shapes look like by simulating a peak generated by the two functions and plotting them:\n\n# Write some code here to generate a peak of each type centred at 1, with a width of 0.1 and amplitude of 10.\n# Tip: the numpy linspace method makes it straightforward to generate an equally-spaced array of numbers for the\n# x-axis data.\n\n# Your code here...\n\n\n# Once you have the two peaks, plot them on the same set of axes to compare them.\n\n# Your code here...\n\n# Uncomment the line below and run the cell to see some code that works\n# %load ../code_snips/gen_peaks.txt\n\n\n\n\nWe now know that the functions can model the shape of Lorentzian and Gaussian peaks. So we should be able to use them to try to fit the peaks in the NMR spectrum. The fitting process optimises the model’s parameters to obtain a calculated spectrum that is as close as possible to the experimentally observed data.\n\nBoth the Lorentzian and Gaussian models have three parameters that can be varied to modify the peak shape: the amplitude, position of the peak’s centre and the peak’s width. Depending on the type of measurement, all three can provide information about the analyte.\n\nWe will use \n\nscipy.optimize.curve_fit to fit the peaks in the spectrum. It uses \n\nnon-linear least squares to fit a function - in this case the Lorentzian we defined - to a set of data.\n\nSee Also\n\nHere is a basic intro to \n\nLeast Squares Optimisation.\n\n","type":"content","url":"/spectrum-analysis#peak-processing-in-scipy","position":7},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Peak finding - getting initial model parameters","lvl2":"Peak processing in scipy"},"type":"lvl3","url":"/spectrum-analysis#peak-finding-getting-initial-model-parameters","position":8},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Peak finding - getting initial model parameters","lvl2":"Peak processing in scipy"},"content":"You might remember that the optimisation needs a set of initial guesses for the parameters to fit the curve. For one or two peaks, that is easily done by hand, but we can use scipy’s signal processing \n\nscipy.signal.find_peaks to do this in Python.\n\nThe find_peaks function locates local maxima in the 1D array it is passed by comparing data points with neighbouring values.\n\nTry running the function on the intensity data of the NMR spectrum.\n\n# Pass the NMR intensities to the find_peaks function and check the results (take a look at the `find_peaks` docs to see what it returns). \n# Note the shape of the array storing the indices of the peaks in the intensity array.\n\npeak_idx, peak_info = find_peaks(nmr_spec[\"intensity\"])\npeak_idx\n\n\n\narray([   1,    3,    5, ..., 6989, 6994, 6998], shape=(2320,))\n\nRunning the peak finding on the intensities without providing any extra constraints results in find_peaks locating over 2000 peaks in the spectrum, rather than the two we see when we plot the spectrum. The additional “peaks” are weak intensity local maxima. The vast majority (all but two) are points that are higher intensity than their surroundings, but not significantly higher than the backgground noise.\n\nfind_peaks can also take other arguments that filter the peaks it finds based on the properties of the peaks. The information it returns will also depend on the arguments passed.\nCheck the \n\ndocs and modify the call to find_peaks to filter the peaks and get the information required as initial guesses for the Lorentzian peaks.\n\n# Modify the call to `find_peaks` below to filter the peaks to just the two real resonances.\n# Your call should include parameters so that the function returns enough peak information to get initial values for \n# peak amplitude, centre and width.\n\npeak_idx, peak_info = find_peaks(nmr_spec[\"intensity\"])\n\n# uncomment the line below and run the cell to load the complete code\n# %load ../code_snips/find_peaks_filter.txt\n\n# Your code here...\n\n\nprint(peak_idx)\npeak_info\n\n\n\nfind_peaks is not aware of the data along the x-axis, i.e. the chemical shift, there is still some work to pull out the peak centres and the width of the peaks in ppm - at the moment, the widths are given in terms of the number of data points.\n\n# Use the indices of the peaks to work out the peak centres in ppm and add them to the peak_info dictionary with the key \"centres\".\n\n# Your code here...\n\n\n# uncomment the line below and run the cell to load some code to do this\n# %load ../code_snips/peak_centres_ppm.txt\n\n\n\n\n# The widths are currently expressed in terms of number of data points, i.e. \" 'widths': array([2.7734915 , 3.57548198]) \" \n# means the FWHM of the first peak is the distance between 2.77 points, which assumes they are evenly spaced.\n# Work out the widths in ppm. Update the widths in the peak_info dictionary to these values.\n# Hint: You can calculate the distance between adjacent rows in a pandas Series using the diff() method.\n\n\n# Your code here...\n\n\n# uncomment the line below and run the cell to load some code to do this\n# %load ../code_snips/widths_ppm.txt\n\n\n\n\n# Check the peak information\npeak_info\n\n\n\n# Tidy up the peak information\n\npeak_info = {key: val for key, val in peak_info.items() if key in [\"peak_heights\", \"widths\", \"centres\"]}\n\n\n\nOK! Finally, let’s try fitting the Lorentzian model to the NMR peaks.\n\n","type":"content","url":"/spectrum-analysis#peak-finding-getting-initial-model-parameters","position":9},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Peak fitting - optimising the model to get precise peak information","lvl2":"Peak processing in scipy"},"type":"lvl3","url":"/spectrum-analysis#peak-fitting-optimising-the-model-to-get-precise-peak-information","position":10},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Peak fitting - optimising the model to get precise peak information","lvl2":"Peak processing in scipy"},"content":"Now you are ready to run the least squares optimisation to fit the curves to the peaks in the spectrum.\n\nFor this simple spectrum, the peaks are well separated so can be fitted separately and the baseline is very low and flat. Check the \n\ncurve_fit docs if you need a reminder of how to call the function, passing in the information from the peak_info dictionary as the initial values for the parameters.\n\nOne thing to note: The amplitude of the fitted curve will be the integrated intensity - the area under the peak. You can estimate the initial area as that of a rectangle of height = peak height and width = peak width.\n\n# Run `curve_fit` for each of the peaks in the NMR spectrum and store the output in the lists popt_list and pcov_list\n\npopt_list = []\npcov_list = []\n\n# Your code here...\n\n\n\n\n# uncomment the line below and run the cell to load some code to do this\n# %load ../code_snips/run_curve_fits.txt\n\n\ndisplay(popt_list, pcov_list)\n\n\n\n\n","type":"content","url":"/spectrum-analysis#peak-fitting-optimising-the-model-to-get-precise-peak-information","position":11},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Assessing the fit","lvl2":"Peak processing in scipy"},"type":"lvl3","url":"/spectrum-analysis#assessing-the-fit","position":12},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Assessing the fit","lvl2":"Peak processing in scipy"},"content":"Check the optimised parameters of the fitted Lorentzian peaks stored in popt_list. These will be [amplitude, centre, width] for each peak.\n\nTake another look at the spectrum. Do these values look reasonable? It might be difficult to tell with the areas, but\nremember the rectangular approximation.\n\nThe covariance matrix for the fitted functions are in pcov_list. The diagonal elements are the variances of the parameters and these can be used to estimate the errors (uncertainties) on the parameters (see \n\ndocs for info).\n\n# Calculate the 1-standard deviation errors for each of the parameters of the peaks. \n# Report the parameter values and the associated errors.\n\n\ndef report_peak_fit(popt, perr):\n    \"\"\"Print peak function parameters and 1-sigma errors\"\"\"\n    report = (f\"Amplitude: {popt[0]:.3f} +/- {perr[0]:.3f}\\n\" \n              f\"Centre: {popt[1]:.4f} +/- {perr[1]:.4f}\\n\"\n              f\"Width: {popt[2]:.4f} +/- {perr[2]:.4f}\\n\")\n\n    print(report)\n\n# Write a comment to explain what this code is doing.\nerrors = [np.sqrt(np.diag(pcov)) for pcov in pcov_list]\n\nfor i, pk in enumerate(popt_list):\n    print(f\"Peak {i+1}\")\n    report_peak_fit(pk, errors[i])\n\n\n\n\nWe can also overlay a spectrum calculated from the optimised peak parameters to see if the fitted peaks look reasonable by eye.\n\n# This function simulates a spectrum using the specified peak shape function and a set of parameters passed as a dictionary.\nfrom simulate import simulate_spectrum\n\n\n\ndef collate_fitted_peak_parameters(popt):\n    \"\"\" Make a dictionary of peak parameters \"\"\"\n\n    parameters = [\"ampl\", \"centre\", \"width\"]\n    return dict(zip(parameters, popt))\n\nfitted_peaks = [collate_fitted_peak_parameters(popt) for popt in popt_list] # assemble a list of dictionaries for peak parameters\n\nlor_spec_x, lor_spec_y = simulate_spectrum(lorentzian, nmr_spec[\"ppm\"], fitted_peaks)\n\n\n\n# y_sigma = nmr_spec[\"intensity\"].std()\n# y_errors = [y_sigma for y in nmr_spec[\"intensity\"]]\n\nnmr_spec.plot(x=\"ppm\", \n              y=\"intensity\",\n              xlabel=\"Chemical shift $\\\\delta$ / ppm\",\n              ylabel=\"Intensity\",\n              kind=\"scatter\",\n              marker=\".\",\n              label=\"Measured\")\n\nplt.plot(lor_spec_x, lor_spec_y, color=\"red\", label=\"Calculated\")\n\nplt.legend()\nplt.xlim((70, 0))\nplt.title(r\"$\\mathregular{^{13}C}$ NMR spectrum of ethanol\")\nplt.show()\n\n\n\nDoes this look like a good fit? Try changing the x-limits to check the fit of the peaks more closely.\n\n","type":"content","url":"/spectrum-analysis#assessing-the-fit","position":13},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Performance metrics","lvl2":"Peak processing in scipy"},"type":"lvl3","url":"/spectrum-analysis#performance-metrics","position":14},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl3":"Performance metrics","lvl2":"Peak processing in scipy"},"content":"We can also use a variety of metrics to assess how well the modelled spectrum fits the experimental data.\n\nR2 (coefficient of determination) is a statistical measure of how well the model explains the variability of the dependent variable (here, the intensity). Its form is quite intuitive and it is defined as follows:R^2 = 1 - \\frac{SSR}{SST}\n\nwhere:\n\nSSR (Sum of Squared Residuals): also called the residual sum of squares (RSS), is the quantity minimised by least square. It measures the total squared differences between the observed values and the predicted values from the model.SSR = \\sum (y_i - \\hat{y}_i)^2\n\nSST (Total Sum of Squares): measures the total variability in the observed data (without any model), based on the mean \\bar{y}:SST = \\sum (y_i - \\bar{y})^2\n\nIf the model fits perfectly,  SSR = 0  and  R^2 = 1 , meaning all variability in the data is explained by the model.\n\nIf the model is no better than simply using the mean  \\bar{y} , then  SSR = SST  and  R^2 = 0 .\n\nIf the model is worse than using the mean (e.g., a bad fit),  R^2  can be negative.\n\nIn least squares fitting, reducing SSR improves the model fit and increases  R^2 , so a good fit has a high  R^2  close to 1.\n\nWe could calculate R2 manually (code in cell below), but scikit-learn’s \n\nmetrics package makes it straightforward to calculate many measures of model performance.\n\n# To calculate r^2:\n\n# residuals = lor_spec_y - nmr_spec[\"intensity\"]\n# squared_residuals = residuals ** 2\n# SSR = squared_residuals.sum()\n# SST = ((nmr_spec[\"intensity\"] - nmr_spec[\"intensity\"].mean())**2).sum()\n# r2 = 1-(SSR/SST)\n# print(r2)\n\n\n\n\n# Import the relevant function from sklearn.metrics to calculate r^2, the coefficient of determination\n\n\n# uncomment the line below and run the cell to load some code to do this\n# %load ../code_snips/r2_sklearn.txt\n\n\n\n\n","type":"content","url":"/spectrum-analysis#performance-metrics","position":15},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl2":"Key Points Summary:"},"type":"lvl2","url":"/spectrum-analysis#key-points-summary","position":16},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl2":"Key Points Summary:"},"content":"Real-World Data Modeling: In many areas of chemistry (e.g., NMR, IR spectroscopy), fitting models to experimental data is crucial for extracting meaningful information, such as peak positions and intensities.\n\nPeak Fitting: Fitting peaks with a model function helps extract more precise information about the data (e.g., determining peak centers, amplitudes, and widths) compared to raw data points.\n\nUse of Least Squares: Least squares optimisation plays a central role in fitting models to experimental data. It helps minimise the difference between the model predictions and the observed data, providing the best-fit parameters for the model.\n\nPractical Considerations: When fitting models, especially in noisy or complex data, it is important to consider the quality of the fit and how well the model reflects the underlying data. This exercise highlights common issues in fitting, such as choosing an appropriate model and evaluating the fit.\n\nIterative Process: Fitting is often an iterative process. You may need to try different initial guesses for the model parameters, refine the model, and evaluate how well the fit matches the data.\n\n","type":"content","url":"/spectrum-analysis#key-points-summary","position":17},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl2":"Things to try and/or consider:"},"type":"lvl2","url":"/spectrum-analysis#things-to-try-and-or-consider","position":18},{"hierarchy":{"lvl1":"Notebook exercise - working with spectral data (or patterns)","lvl2":"Things to try and/or consider:"},"content":"Write some code to add a line that shows the residuals as a difference plot below the final graph of the experimental and calculated data.\n\nRepeat the fit using the Gaussian function. Is this a better or worse model for the NMR peak shapes?\n\nWhat happens if the initial guesses for the peak function’s parameters are not close to the actual values? Try fitting the peaks with one of the centres far from the real location. What happens?\n\nWe treated the spectrum using a very generic peak fitting process. Specialised NMR analysis libraries have methods to deal with more complex data much more efficiently (but many will still be using least squares underneath). Can you think of what additional complexities 1H NMR might pose, for example? What issues might arise if peaks are much closer together?\n\nThe 7000 points of the NMR data have effectively been reduced to six numbers. This poses some questions about how data is stored, reported and used for further analysis. What factors might be important when making those decisions? Are there any disadvantages of only having the peak information available? Would the choice be different (how, why) in different scenarios?\n\nYou can see how it might be possible to automate this process for NMR spectra and other types of measured datasets. What steps would be needed now to interpret the information to translate it to knowledge about the molecule? How straightforward is this to automate?","type":"content","url":"/spectrum-analysis#things-to-try-and-or-consider","position":19},{"hierarchy":{"lvl1":"Chemical Data, Discovery and Design"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Chemical Data, Discovery and Design"},"content":"Canvas site\n\nThis book hopefully acts as the main resource for content for the first half of CHEM502.\n\nThis book was originally written by Sam Chong and is now maintained and updated by Joe Forth. Please let me (Joe - \n\nj​.forth@liverpool​.ac​.uk) know if you spot any errors, cannot access any of the expected content, have suggestions for additional content or resources to include.\n\nYou can access the GitHub repository containing the markdown files and notebooks \n\nhere. The code is available in the book directory and is broken down into topics.\n\nThere is a readme and environment yaml/requirements files in the root directory of the repository which should hopefully enable you create a conda environment in which you can run the notebooks.\n\nCHEM502 - Chemical Data, Discovery, and Design\n\nChemical Data\n\nMolecular descriptors and similarity\n\nNotebook exercise - molecular fingerprints\n\nNotebook exercise - working with spectral data (or patterns)\n\nLeast squares optimisation\n\nWarning\n\nThe book is under active development. Content will be added throughout the semester.","type":"content","url":"/","position":1}]}